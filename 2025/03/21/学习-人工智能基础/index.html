<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/%E5%A4%B4%E5%83%8F1.jpg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/%E5%A4%B4%E5%83%8F1.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/%E5%A4%B4%E5%83%8F1.jpg">
  <link rel="mask-icon" href="/images/%E5%A4%B4%E5%83%8F1.jpg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"lwlaymh.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.23.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="常用激活函数  Sigmoid函数 tanh函数 ReLU函数 Leaky ReLU函数 Softmax函数  损失函数  Least Square Cross Entropy  神经网络实现  梯度下降法 误差反向传播  卷积神经网络(CNN)  感受野计算 池化(Pooling) 常见卷积架构  AlexNet VGG ResNet">
<meta property="og:type" content="article">
<meta property="og:title" content="人工智能基础">
<meta property="og:url" content="http://lwlaymh.github.io/2025/03/21/%E5%AD%A6%E4%B9%A0-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80/index.html">
<meta property="og:site_name" content="LWLAymh的备忘录">
<meta property="og:description" content="常用激活函数  Sigmoid函数 tanh函数 ReLU函数 Leaky ReLU函数 Softmax函数  损失函数  Least Square Cross Entropy  神经网络实现  梯度下降法 误差反向传播  卷积神经网络(CNN)  感受野计算 池化(Pooling) 常见卷积架构  AlexNet VGG ResNet">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-03-21T13:47:17.964Z">
<meta property="article:modified_time" content="2025-04-14T16:57:17.290Z">
<meta property="article:author" content="LWLAymh">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://lwlaymh.github.io/2025/03/21/%E5%AD%A6%E4%B9%A0-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://lwlaymh.github.io/2025/03/21/%E5%AD%A6%E4%B9%A0-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80/","path":"2025/03/21/学习-人工智能基础/","title":"人工智能基础"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>人工智能基础 | LWLAymh的备忘录</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous" defer></script>
<script src="/js/third-party/search/local-search.js" defer></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">LWLAymh的备忘录</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">我永远喜欢言和和！</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">常用激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.1.</span> <span class="nav-text">Sigmoid函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.2.</span> <span class="nav-text">tanh函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.3.</span> <span class="nav-text">ReLU函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.4.</span> <span class="nav-text">Leaky ReLU函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.5.</span> <span class="nav-text">Softmax函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.</span> <span class="nav-text">损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">2.1.</span> <span class="nav-text">Least Square</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">2.2.</span> <span class="nav-text">Cross Entropy</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.</span> <span class="nav-text">神经网络实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.1.</span> <span class="nav-text">梯度下降法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.2.</span> <span class="nav-text">误差反向传播</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">4.</span> <span class="nav-text">卷积神经网络(CNN)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">4.1.</span> <span class="nav-text">感受野计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">4.2.</span> <span class="nav-text">池化(Pooling)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">4.3.</span> <span class="nav-text">常见卷积架构</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">4.3.1.</span> <span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">4.3.2.</span> <span class="nav-text">VGG</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">4.3.3.</span> <span class="nav-text">ResNet</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">4.3.4.</span> <span class="nav-text">SqueezeNet</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">4.3.5.</span> <span class="nav-text">MobileNet</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">4.3.6.</span> <span class="nav-text">ShuffleNet</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">4.3.7.</span> <span class="nav-text">反卷积</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="LWLAymh"
      src="/images/%E5%A4%B4%E5%83%8F1.jpg">
  <p class="site-author-name" itemprop="name">LWLAymh</p>
  <div class="site-description" itemprop="description">不过是白日梦里一瞬息</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">61</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://minyuchengmin.github.io/" title="https:&#x2F;&#x2F;minyuchengmin.github.io" rel="noopener" target="_blank">ycm神仙的blog</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://wminus.github.io/" title="https:&#x2F;&#x2F;wminus.github.io" rel="noopener" target="_blank">麦麦的blog</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://shanlunjiajian.github.io/" title="http:&#x2F;&#x2F;shanlunjiajian.github.io" rel="noopener" target="_blank">qyc的blog</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.cnblogs.com/do-while-true" title="https:&#x2F;&#x2F;www.cnblogs.com&#x2F;do-while-true" rel="noopener" target="_blank">dwt的blog</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://lwlaymh.github.io/2025/03/21/%E5%AD%A6%E4%B9%A0-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E5%A4%B4%E5%83%8F1.jpg">
      <meta itemprop="name" content="LWLAymh">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LWLAymh的备忘录">
      <meta itemprop="description" content="不过是白日梦里一瞬息">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="人工智能基础 | LWLAymh的备忘录">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          人工智能基础
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-03-21 21:47:17" itemprop="dateCreated datePublished" datetime="2025-03-21T21:47:17+08:00">2025-03-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-04-15 00:57:17" itemprop="dateModified" datetime="2025-04-15T00:57:17+08:00">2025-04-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E5%AD%A6%E8%AF%BE%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">大学课程</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><!-- toc -->
<ul>
<li><a href="#常用激活函数">常用激活函数</a>
<ul>
<li><a href="#sigmoid函数">Sigmoid函数</a></li>
<li><a href="#tanh函数">tanh函数</a></li>
<li><a href="#relu函数">ReLU函数</a></li>
<li><a href="#leaky-relu函数">Leaky ReLU函数</a></li>
<li><a href="#softmax函数">Softmax函数</a></li>
</ul></li>
<li><a href="#损失函数">损失函数</a>
<ul>
<li><a href="#least-square">Least Square</a></li>
<li><a href="#cross-entropy">Cross Entropy</a></li>
</ul></li>
<li><a href="#神经网络实现">神经网络实现</a>
<ul>
<li><a href="#梯度下降法">梯度下降法</a></li>
<li><a href="#误差反向传播">误差反向传播</a></li>
</ul></li>
<li><a href="#卷积神经网络cnn">卷积神经网络(CNN)</a>
<ul>
<li><a href="#感受野计算">感受野计算</a></li>
<li><a href="#池化pooling">池化(Pooling)</a></li>
<li><a href="#常见卷积架构">常见卷积架构</a>
<ul>
<li><a href="#alexnet">AlexNet</a></li>
<li><a href="#vgg">VGG</a></li>
<li><a href="#resnet">ResNet</a></li>
<li><a href="#squeezenet">SqueezeNet</a></li>
<li><a href="#mobilenet">MobileNet</a></li>
<li><a href="#shufflenet">ShuffleNet</a></li>
<li><a href="#反卷积">反卷积</a></li>
</ul></li>
</ul></li>
</ul>
<!-- tocstop -->
<span id="more"></span>
<h3><span id="常用激活函数">常用激活函数</span></h3>
<h4><span id="sigmoid函数">Sigmoid函数</span></h4>
<p><span class="math inline">\(f(x)=\frac{1}{1+e^{-x}}:(-\infty,+\infty)\to(0,1)\)</span>.</p>
<p><span class="math inline">\(f&#39;(x)=f(x)(1-f(x))\)</span>.</p>
<h4><span id="tanh函数">tanh函数</span></h4>
<p><span class="math inline">\(f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}:(-\infty,+\infty)\to(-1,1)\)</span>.</p>
<p><span class="math inline">\(f&#39;(x)=1-f^2(x)\)</span>.</p>
<h4><span id="relu函数">ReLU函数</span></h4>
<p><span class="math inline">\(f(x)=\max(0,x):(-\infty,+\infty)\to(0,+\infty)\)</span>.</p>
<h4><span id="leaky-relu函数">Leaky ReLU函数</span></h4>
<p><span class="math inline">\(f(x)=\max(\alpha
x,x),0&lt;\alpha&lt;1:(-\infty,+\infty)\to(-\infty,+\infty)\)</span>.</p>
<h4><span id="softmax函数">Softmax函数</span></h4>
<p><span class="math inline">\(f(x_i)=\frac{e^{x_i}}{\sum_j
e^{x_j}}\)</span>.</p>
<h3><span id="损失函数">损失函数</span></h3>
<h4><span id="least-square">Least Square</span></h4>
<p>即<span class="math inline">\(\arg \min
\sum_{i=1}^n(f(x_i)-y_i)^2=\arg \min(A\beta-Y\mid
A\beta-Y)\)</span>.用最小二乘法取<span class="math inline">\(\hat
\beta=(A^TA)^{-1}A^TY\)</span>.</p>
<h4><span id="cross-entropy">Cross Entropy</span></h4>
<p>用错误的分布<span class="math inline">\(q\)</span>来表示真实分布<span class="math inline">\(p\)</span>的样本,则平均编码长度应该是: <span class="math display">\[
H(p,q)=\sum_i p(i)\log(\frac{1}{q(i)})=-\sum_i p(i)\log{q(i)}
\]</span> 此为交叉熵.</p>
<p>特别地,当最终样本只有两个的时候,例如Logistical
Regression问题,可以写成: <span class="math display">\[
H=-(y\log a+(1-y)\log(1-a))\\
\frac{\partial H}{\partial a}=-(\frac{y}{a}-\frac{1-y}{1-a})
\]</span></p>
<p>那如果有多个呢?考虑直接对归一化条件作偏导,先有: <span class="math display">\[
H(p,q)=-\sum_i p_i(\log{q_i}-\log(\sum_j q_j))\\
\frac{\partial H(p,q)}{\partial q_k}=-\frac{p_k}{q_k}+\frac{\sum
p_k}{\sum_j q_j}\\
=-\frac{p_k}{q_k}+1
\]</span> 再乘以softmax那里的<span class="math inline">\(q_k\)</span>,得到<span class="math inline">\(-p_k+q_k\)</span>.</p>
<h3><span id="神经网络实现">神经网络实现</span></h3>
<p>通过若干隐藏层,假设最后的输出层为第<span class="math inline">\(L\)</span>层,则:</p>
<ol type="1">
<li>对于第<span class="math inline">\(l\)</span>层,取<span class="math inline">\(\vec z_l=(W_l)^t \vec a_{l-1}+\vec
b_l\)</span>.这里对<span class="math inline">\(W_l\)</span>作转置的目的是写代码的时候需要用行向量.</li>
<li>对于第<span class="math inline">\(l\)</span>层,取<span class="math inline">\(\vec a_l=f(\vec
z_l)\)</span>,这里意味着将每一个分量对<span class="math inline">\(f\)</span>操作.</li>
<li>对于最终答案,取误差<span class="math inline">\(\mathcal
L=\frac{1}{m}(\vec y-\vec a_L\mid \vec y-\vec a_L)\)</span>.</li>
</ol>
<h4><span id="梯度下降法">梯度下降法</span></h4>
<p>换言之就是让<span class="math inline">\(w:=w-\alpha\frac{\partial
\mathcal{L}}{\partial w}\)</span>,其中<span class="math inline">\(\alpha\)</span>是一个选定的小常数,也可以采用类似模拟退火的方式动态决定.事实上可以把各个位置分开,写作<span class="math inline">\(w_j:=w_j-\alpha
\frac{\partial\mathcal{L}}{\partial w_j}\)</span>.</p>
<p>另外,虽然是这么写,应当见到去掉下标<span class="math inline">\(k\)</span>的记号仍然合理,无非是逐分量做此操作,因此下面如无特殊说明,运算均采用逐分量运算.例如可以定义<span class="math inline">\(\vec a\circ \vec
b\)</span>为两个向量逐分量相乘后得到的新向量,为表区分用<span class="math inline">\(\times\)</span>表示正常的矩阵乘法.甚至采取<span class="math inline">\((\vec a-\vec
y)^2\)</span>表示其自点积.坦白而言,笔者对此符号相当无奈,可也想不出什么更好的写法了.但总之这种写法总是强于部分参考资料上所将下标放上面的写作<span class="math inline">\(a^L\)</span>的做法.笔者所能维持的精神数院人的唯一做法也只能是在下面加上向量符号,藉此泄愤.</p>
<p>顺便一提,应当见到<span class="math inline">\(\times\)</span>和<span class="math inline">\(\circ\)</span>这两种运算比较随意,用线性映射来理解,你这个$W$任意作用在一个向量上就行.</p>
<h4><span id="误差反向传播">误差反向传播</span></h4>
<p>既然要用梯度下降法,就应该把每一层的偏导都求出来.然而<span class="math inline">\(\mathcal
L\)</span>是最后一层的结果,因此应该用链式法则一路求出前面的偏导.</p>
<p>更具体地,不妨设误差函数选的是<span class="math inline">\((\vec a-\vec
y)^2\)</span>,激活函数选的是cross entropy有:</p>
<ol type="1">
<li><span class="math inline">\(\frac{\partial{\mathcal L}}{\partial
\vec a_{L}}=(\vec a_{L}-\vec
y)\)</span>.(如若选择不同的误差函数,这里作适当变化)</li>
<li><span class="math inline">\(\frac{\partial \vec a_{l}}{\partial \vec
z_{l}}=f&#39;( \vec z_{l})=\vec a_{l}\circ (1-\vec
a_{l})\)</span>.(如若选取不同的激活函数,这里作适当变化)</li>
<li><span class="math inline">\(\frac{\partial \vec z_{l+1}}{\partial
\vec z_{l}}=\frac{\partial \vec z_{l+1}}{\partial \vec
a_{l}}\frac{\partial \vec a_{l}}{\partial \vec z_{l}}=(
W_{l+1})^t\times(\vec a_l\circ (1-\vec a_l))\)</span>.</li>
<li><span class="math inline">\(\frac{\partial \vec z_{l}}{\partial
W_{l}}=(\vec
a_{l-1})\)</span>.结果理应是一个矩阵,其实就是这个列向量不断复制若干遍,或者写成<span class="math inline">\((\vec a_{l-1})^tM(1)\)</span>,其中<span class="math inline">\(M(1)\)</span>是全<span class="math inline">\(1\)</span>矩阵.</li>
<li><span class="math inline">\(\frac{\partial \vec z_{l}}{\partial \vec
b_{l}}=1\)</span>.</li>
</ol>
<p>我们应当见到:</p>
<p>不妨设<span class="math inline">\(\delta_l=\frac{\partial {\mathcal
L}}{\partial \vec z_l}\)</span>.见到:</p>
<ol type="1">
<li><span class="math inline">\(\delta_L=\frac{\partial\mathcal
L}{\partial \vec z_L}=\frac{\partial\mathcal L}{\partial \vec
a_L}\frac{\partial\vec a_L}{\partial \vec z_L}=(\vec a_{L}-\vec y)\circ
\vec a_{L}\circ (1-\vec
a_{L})\)</span>.前者会因为误差函数的选取而改变,后者会因为激活函数的选取而改变.</li>
<li><span class="math inline">\(\delta_l=\frac{\partial {\mathcal
L}}{\partial \vec z_l}=\delta_{l+1}\frac{\partial \vec z_{l+1}}{\partial
\vec z_l}=\delta_{l+1}\circ  (W_{l+1})^t\times (\vec a_{l}\circ (1-\vec
a_{l}))\)</span>.</li>
<li><span class="math inline">\(\frac{\partial \mathcal L}{\partial
W_{l}}=\frac{\partial \mathcal L}{\partial \vec z_{l}}\frac{\partial
\vec z_l}{\partial W_{l}}=\delta_l\times a_{l-1}^t\)</span>.</li>
<li><span class="math inline">\(\frac{\partial \mathcal L}{\partial \vec
b_l}=\delta_l\)</span>.</li>
</ol>
<p>如此以上更新即可.</p>
<h3><span id="卷积神经网络cnn">卷积神经网络(CNN)</span></h3>
<p>神经网络受矩阵乘法的限制,导致对于真实的尺寸巨大的图像难以快速识别,因此产生了卷积神经网络的概念,大概有以下特征:</p>
<ol type="1">
<li>空间上权值共享:不同位置使用同一个卷积核(滤波器)</li>
<li>稀疏链接:每一层只链接前一层的感受野.</li>
<li>等变表示:卷积神经网络有某种平移不变性.</li>
</ol>
<p>对于2D卷积,其公式如下: <span class="math display">\[
S_{r,c}=(X*W)_{r,c}=\sum_i\sum_j X_{r+i,c+j}\times w_{i,j}
\]</span> 其中<span class="math inline">\(W\)</span>是卷积核,<span class="math inline">\(X\)</span>是输入图像,<span class="math inline">\(S\)</span>是输出的结果.如果一个图像有多个通道(比如色彩层之类的),每个通道上都需要应用一个卷积核.</p>
<p>下面引入一些名词:</p>
<ol type="1">
<li>input size:输入图像的尺寸.</li>
<li>padding:填充的像素数.</li>
<li>filter size:卷积核的尺寸.有时也写作两个变量:filter height和filter
width.3D卷积还会有一个filter depth的变量.</li>
<li>stride:步长.</li>
<li>output size:卷积后输出的尺寸.有时也写作feature size.</li>
<li>input channels:输入图像的通道数.</li>
<li>n filters:卷积核的数量.</li>
<li>dilation rate:膨胀率,用于空洞卷积.膨胀率为<span class="math inline">\(d\)</span>的时候,卷积核中间会插入<span class="math inline">\(d-1\)</span>个<span class="math inline">\(0\)</span>间隔.</li>
</ol>
<h4><span id="感受野计算">感受野计算</span></h4>
<p>先看output
size的计算,容易见到,其各个维度方面计算是独立的.只要对于单个维度算出卷积核在上面移动的次数,最后将不同维度相乘即可.</p>
<p>对于单个维度,这个维度的移动次数应该是: <span class="math display">\[
\text{output\_size}=\lceil\frac{\text{input\_size}+2\times
\text{padding}-\text{filter\_size}+1}{\text{stride}}\rceil\\
=\lfloor\frac{\text{input\_size}+2\times
\text{padding}-\text{filter\_size}}{\text{stride}}\rfloor+1\\
\]</span> 这个公式相当容易理解,原因是<span class="math inline">\(\text{stride}=1\)</span>的时候,上面恰好是移动的次数,而<span class="math inline">\(\text{stride}\)</span>变化的时候,当然要拿到一个上取整.</p>
<p>至于所谓的空洞卷积,只需在上面的基础上改<span class="math inline">\(\text{filter\_size}\)</span>就好.</p>
<p>至于乘法操作,每得到一个<span class="math inline">\(\text{output}\)</span>当然都会需要<span class="math inline">\(\text{filter\_size}\)</span>次乘法操作.</p>
<h4><span id="池化pooling">池化(Pooling)</span></h4>
<p>池化操作它没有一个可学习的参数,只是对输入数据进行固定的操作.简单来说就是降低输入的规模,以实现更好的鲁棒性以及提高效率.</p>
<p>常见的池化操作包括:</p>
<ol type="1">
<li>MaxPooling:取区域内的最大值.</li>
<li>MeanPooling:取区域内的平均值.</li>
<li>PyramidPooling:多次进行尺度不同的池化.</li>
</ol>
<h4><span id="常见卷积架构">常见卷积架构</span></h4>
<h5><span id="alexnet">AlexNet</span></h5>
<p>首次引入ReLU激活函数,Dropout
技术,以及数据增强,提高了模型的训练效率和泛化能力.</p>
<p>采用了<span class="math inline">\(8\)</span>层深的网络结构,证明了深度网络的潜力.</p>
<h5><span id="vgg">VGG</span></h5>
<p>开始堆叠小尺寸的卷积核,获得与大卷积核相似的感受野的同时可以增加网络深度.</p>
<h5><span id="resnet">ResNet</span></h5>
<p>引入残差的概念,直接将输入数据累加(跳跃连接)到最后的输出中,这样网络学习的实际上是输入和输出之间的残差,从而提高了网络学习能力.</p>
<h5><span id="squeezenet">SqueezeNet</span></h5>
<p>SqueezeNet的基本构建单元是Fire模块.Fire模块由一个squeeze层和一个expand层组成.squeeze层使用<span class="math inline">\(1\times1\)</span>卷积核减少通道数,而expand层则使用<span class="math inline">\(1\times 1\)</span>和<span class="math inline">\(3\times
3\)</span>卷积核增加通道数.这种设计有效地减少了参数数量和计算量.</p>
<h5><span id="mobilenet">MobileNet</span></h5>
<ol type="1">
<li>深度卷积:在这个操作中,每个输入通道独立地进行卷积,这意味着在进行卷积时,不同通道之间没有交互.这样可以减少计算量和参数数量.</li>
<li>逐点卷积:逐点卷积使用<span class="math inline">\(1\times
1\)</span>的卷积核,它作用在深度卷积的输出上,将不同通道的信息整合在一起.逐点卷积可以减少参数数量,同时保持较高的性能.</li>
</ol>
<h5><span id="shufflenet">ShuffleNet</span></h5>
<ol type="1">
<li>组卷积(Group
Convolution):将通道分成几个组,并使用不同的卷积神经网络层执行标准卷积.</li>
<li>打乱层(Shuffle layer):通过对通道进行洗牌,将不同组的信息合并.</li>
</ol>
<h5><span id="反卷积">反卷积</span></h5>
<p>也就是将较小的数据特征图扩大到较大的尺寸.有的时候也把这个操作说成上采样.</p>
<ol type="1">
<li>插值步骤(Interpolation
Step):首先,在输入特征图的元素之间插入零,增加特征图的尺寸.</li>
<li>卷积步骤(Convolution
Step):接下来,对扩大后的特征图应用一个标准的卷积操作.此步骤相当于在扩大的特征图上滑动卷积核,计算卷积输出.</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/03/10/%E9%9A%8F%E7%AC%94-%E8%AA%93/" rel="prev" title="誓">
                  <i class="fa fa-angle-left"></i> 誓
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/03/29/%E4%B9%A0%E4%BD%9C-%E8%96%84%E8%8D%B7%E5%85%BB%E6%8A%A4%E6%97%A5%E5%BF%97/" rel="next" title="薄荷养护日志">
                  薄荷养护日志 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">LWLAymh</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

  <!-- 音乐播放器 -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.0/dist/APlayer.min.css">
  <script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.0/dist/APlayer.min.js"></script>
  <div id="aplayer" class="aplayer" data-id="7637648380" data-server="netease" data-type="playlist" data-fixed="true" data-listfolded="true" data-order="random" data-theme="#F58EA8"></div>
  <script src="https://unpkg.com/meting@1.2/dist/Meting.min.js"></script>
  <!-- 音乐播放器 end -->
</body>
</html>
